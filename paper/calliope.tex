\documentclass{article}


\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Black-box adversarial examples in the real world}
\author{
\textbf{Hristo Todorov} \\
MSHS 'Prof. Emanuil Ivanov' \\
Kyustendil, Bulgaria \\
\texttt{httodoroff@gmail.com}
\and
\textbf{Kristian Georgiev} \\
Massachusetts Institute of Technology \\
Cambridge, Massachusetts \\
\texttt{krisgrg@mit.edu}
}
\date{July 2020}

\begin{document}

\maketitle

\section{Introduction}
Artificial intelligence is one of the fastest-growing fields these days. In recent years, machine learning models are being applied to a wide variety of tasks such as image classification \cite{alexnet}, voice recognition \cite{chiu2017stateoftheart}, medical analysis \cite{medical}, stock market prediction, etc. Self-driving cars are also quite a famous concept that combines numerous kinds of machine learning techniques in order to create a complex system that is capable of dealing with unexpected scenarios. As neural networks become more and more important part of our lives, the question of whether they are reliable enough arises. Unfortunately, as for now, it has been discovered that there are many serious flaws even in the state-of-the-art models. Because of that, the creation of secure, robust, and reliable algorithms is a vital problem that needs to be solved before integrating machine learning more deeply into our lifestyle, supposing that we want to be able to trust them.

Machine learning models are vulnerable to several different kinds of attacks such as data poisoning, trojaning, membership inference, and backdooring. The most common type of attack is called evasion or adversarial attack \cite{goodfellow2014explaining}. The goal of an evasion attack is to manipulate the predictions of a model. Adversarial examples are inputs with some perturbations applied to them (usually these perturbations are quite small and they are almost completely imperceptible). When they are fed to the model, it misclassifies them. Various fields, such as image classification, voice recognition, and reinforcement learning are vulnerable to evasion attacks. It has been shown that they could also be executed in the real world \cite{kurakin2016adversarial, athalye2017synthesizing}. An adversarial example may be used either to make the model return any wrong output (untargeted attack) or even to shift its prediction to a desired state (targeted attack). As a result, one can obtain control over the output of a model by executing a targeted attack, which might be used for example as a tool for deceiving and penetrating facial recognition systems. Therefore, the task of building robust and reliable models that are resistant to evasion attacks is a major problem in the areas of machine and deep learning.

\section{Related work}
The problem was officially defined in 2013 \cite{szegedy2013intriguing, Biggio_2013}, however, the phenomenon had been mentioned before \cite{dalvi, Kolcz2009FeatureWF, barreno}. At first, it did not attract much attention, because artificial neural networks were not able to produce plausible results and compete with humans at a task such as image classification back then. Nevertheless, in the following years, their performance considerably increased, and the current state-of-the-art image classification networks outperform humans. Therefore, the problem of evasion vulnerability became a hot topic in research. There are numerous work investigating different kinds of adversarial attacks \cite{goodfellow2014explaining, ilyas2018prior} and defenses \cite{aleks2017deep, xiao2018training}. It has been demonstrated that they can transfer in the physical world both as plain photos\cite{kurakin2016adversarial} and 3D objects \cite{athalye2017synthesizing}.

\newpage

\section{Adversarial examples}
An adversarial example is an input to a machine learning model that has been cautiously altered so that it can successfully fool the model. The perturbations applied to the original image can be adding noise, changing the value of a single pixel, rotating it, and more. Let us look at a artificial neural network, represented as a function $f_{\theta}:\chi \to \mathbb{R}^m$, where $\theta$ are the parameters of the model, $\Chi$ is a batch of images $[x_{1},x_{2},x_3{},...x_{n}] \in \chi$ and $m$ is the number of classes being predicted. We calculate $[f_{theta}(x_{1}),f_{theta}(x_{2}),f_{theta}(x_{3}),...f_{theta}(x_{n})] \equiv [y_{1},y_{2},y_{3},...y_{n}]$ - the predictions of the model regarding the original images ( As for now, we will only consider the process of creating an adversarial examples by adding noise to the original images. For each $x_{k}$ ($k \in [1, 2, 3,...n])$ we create $\hat{x}_{k} = x_{k} + \delta$, where $\delta$ (our noise) is a tensor with the same size as $x_{k}$. Furthermore, we define a loss function $l(f_{theta}(x_{k}, y_{k}))$

\begin{figure}[ht]
    \centering
    \subfigure[The original image, which is correctly classified as a dog (Bouvier des Flandres)]{\includegraphics[width=5.5cm]{dog_ski_original.jpg}}
    \hfill
    \subfigure[The adversarial example of the original image, which is classified as a ski mask]{\includegraphics[width=5.5cm]{dog_ski_original.jpg}}
    \caption{Example of an adversarial attack: a small amount of special noise is added to the original image in order to create a working adversarial example}
\end{figure}

\newpage
\bibliographystyle{plain}
\bibliography{references}

\end{document}